<!doctype html>
<html lang="en">


<!-- === Header Starts === -->
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Multimodal Motion Prediction with Stacked Transformers</title>

  <link href="./assets/bootstrap.min.css" rel="stylesheet">
  <link href="./assets/font.css" rel="stylesheet" type="text/css">
  <link href="./assets/style.css" rel="stylesheet" type="text/css">
</head>
<!-- === Header Ends === -->


<body>


<!-- === Home Section Starts === -->
<div class="section">
  <!-- === Title Starts === -->
  <div class="header">
    <div class="logo">
      <a href="https://genforce.github.io/" target="_blank"><img src="./assets/deciforce.png"></a>
    </div>
    <div class="title", style="padding-top: 25pt;">  <!-- Set padding as 10 if title is with two lines. -->
      Multimodal Motion Prediction with Stacked Transformers
    </div>
  </div>
  <!-- === Title Ends === -->
  <div class="author">
      <a href="#" target="_blank"> Yicheng Liu<sup>1*</sup>,&nbsp;</a>
      <a href="#" target="_blank"> Jinghuai Zhang<sup>2*</sup>,&nbsp;</a>
      <a href="#" target="_blank"> Liangji Fang<sup>2</sup>,</a>
      <a href="https://scholar.google.com.hk/citations?user=yQAPc88AAAAJ&hl=zh-CN" target="_blank">Qinhong Jiang<sup>2</sup>,</a>
      <a href="http://bzhou.ie.cuhk.edu.hk/"
        target="_blank">Bolei Zhou<sup>1</sup></a>
    </div>
  <div class="institution">
    The Chinese Univsersity of Hong Kong<sup>1</sup>, SenseTime Research<sup>2</sup>
  </div>
  <div class="conference">
    Computer Vision and Pattern Recognition (CVPR), 2021
  </div>
  <div class="link" style="font-size: 14pt;">
    <a href="https://arxiv.org/pdf/2103.11624.pdf" target="_blank">[Paper]</a>&nbsp;
    <a href="https://github.com/decisionforce/mmTransformer" target="_blank">[Code]</a>
  </div>
  <div class="title">
    <img src="./assets/teasar.png" width="550" align="middle">
  </div>
</div>
<!-- === Home Section Ends === -->


<!-- === Overview Section Starts === -->
<div class="section">
  <div class="title">Overview</div>
  <div class="body">
    We propose a novel end-to-end motion prediction framework (mmTransformer) for multimodal motion prediction. Firstly, we utilize stacked transformers architecture to incoporate multiple channels of contextual information, and model the multimodality at feature level with a set of trajectory proposals. Then, we induce the multimodality via a tailored region-based training strategy. By steering the training of proposals, our model effectively mitigates the complexity of motion prediction while ensuring the multimodal outputs. 
  </div>
</div>
<!-- === Overview Section Ends === -->


<!-- === Overview Section Starts === -->
<div class="section">
  <div class="title">Method</div>
  <div class="title">
    <img src="./assets/Figure1.png" width="800">
  </div>
  <div class="body">
    The proposed architecture of <font size="4" color="red">mmTransformer (MultiModal Transformer)</font>. The backbone is composed of stacked transformers, which aggregate the contextual information progressively. Proposal feature decoder further generates the trajectory and confidence score for each learned trajectory proposal through the trajectory generator and selector, respectively.
  </div>
  <div class="title">
    <img src="./assets/region-based-training.png" width="500">
  </div>
    Overview of the <font size="4" color="blue">Region-based Training Strategy (RTS)</font>. We distribute each proposal to one of the M regions. These proposals, shown in colored rectangles, learn corresponding proposal feature through the stacked transformers. In training stage, we select the proposals assigned to the region where the GT endpoints locate, generate their trajectories and confidence scores, and then calculate the losses for them.
  <div class="title">
    <img src="./assets/Figure5.png" width="700">
  </div>
    Visualization of the multimodal prediction results on Argoverse validation set. We utilize all trajectory proposals to generate multiple trajectories for each scenario and visualize all the predicted endpoints (black background) in the figures. Colored points indicate the prediction results of a specific group of proposals (after filtering by score). We observe that the endpoints generated by each group of regional proposals are within the associated region.
</div>
<!-- === Overview Section Ends === -->


<!-- === Result Section Starts === -->
<div class="section">
  <div class="title">Results</div>
  <div class="body">
    Qualitative comparison between mmTransformer (6 proposals) and mmTransformer+RTS (36 proposals):
    <!-- Adjust the number of rows and columns (EVERY project differs). -->
    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./assets/Figure4.png" width="90%"></td>
      </tr>
    </table>
    Demo video link: 
    <br>
<!--     (Note: For each agent, <font size="4" color="green">best prediction result</font> is visualized) -->
    <!-- Adjust the frame size based on the demo (EVERY project differs). -->
    <div style="position: relative; padding-top: 50%; margin: 20pt 0; text-align: center;">
      <iframe src="https://www.youtube.com/embed/oUZQBGOEBMg" 
              title="Demo video of mmTransformer."
              frameborder=0
              style="position: absolute; top: 2.5%; left: 2.5%; width: 95%; height: 100%;"
              allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
              allowfullscreen></iframe>
    </div>
    <div> 
      Demo video of multimodal motion prediction by mmTransformer. For each moving vehicle nearby the ego car, <font size="4" color="green">three plausible future trajectories</font> are visualized.
    </div>
    <br>
    Also can be found in:
    <a href="https://www.bilibili.com/video/BV11X4y1374s/" target="_blank">[bilibili]</a>
  </div>
</div>
<!-- === Result Section Ends === -->


<!-- === Reference Section Starts === -->
<div class="section">
  <div class="bibtex">BibTeX</div>
<pre>
@article{liu2021multimodal,
  title={Multimodal Motion Prediction with Stacked Transformers},
  author={Liu, Yicheng and Zhang, Jinghuai and Fang, Liangji and Jiang, Qinhong and Zhou, Bolei},
  journal={Computer Vision and Pattern Recognition},
  year={2021}
}
</pre>

  <!-- BZ: we should give other related work enough credits, -->
  <!--     so please include some most relevant work and leave some comment to summarize work and the difference. -->
  <div class="ref">Related Work</div>
  <div class="citation">
    <div class="image"><img src="./assets/tnt.png"></div>
    <div class="comment">
      <a href="https://arxiv.org/pdf/2008.08294.pdf" target="_blank">
        Hang Zhao, Jiyang Gao, Tian Lan, Chen Sun, Benjamin Sapp, Balakrishnan Varadarajan, Yue Shen, Yi Shen, Yuning Chai, Cordelia Schmid, Congcong Li, Dragomir Anguelov.
        TNT: Target-driveN Trajectory Prediction.
        CoRL 2020.</a><br>
      <!-- <b>Comment:</b>
      This is a short comment. -->
    </div>
  </div>
  <div class="citation">
    <div class="image"><img src="./assets/vec2.png"></div>
    <div class="comment">
      <a href="https://arxiv.org/pdf/2005.04259.pdf" target="_blank">
        Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir Anguelov, Congcong Li, Cordelia Schmid.
        VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation.
        CVPR 2020.</a><br>
      <!-- <b>Comment:</b>
      This is a long comment. This comment is just used to test how long comments can fit the template. -->
    </div>
  </div>
  <div class="citation">
    <div class="image"><img src="./assets/TP.png"></div>
    <div class="comment">
      <a href="https://arxiv.org/pdf/2004.12255.pdf" target="_blank">
        Liangji Fang, Qinhong Jiang, Jianping Shi, Bolei Zhou.
        TPNet: Trajectory Proposal Network for Motion Prediction.
        CVPR 2020.</a><br>
      <!-- <b>Comment:</b>
      This is a long comment. This comment is just used to test how long comments can fit the template. -->
    </div>
  </div>
</div>
<!-- === Reference Section Ends === -->


</body>
</html>
